{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Defining how we assess performance\n",
    "\n",
    "- Measuring loss\n",
    " - Loss function: $L(y, f_{\\hat{w}}(X))$\n",
    "   - $y$: actual value\n",
    "   - $\\hat{f(X)}$: predicted value $\\hat{y}$\n",
    " - exmaples:\n",
    "   - Absolute error: $L(y, f_{\\hat{w}}(X)) = |y-f_{\\hat{w}}(X)|$\n",
    "   - Absolute error: $L(y, f_{\\hat{w}}(X)) = (y-f_{\\hat{w}}(X))^2$\n",
    " - \"Remember that all models are worng; the practical question is how wrong do they have to be to not be useful.\" George Box, 1987"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3 measures of loss and their trends with model complexity\n",
    "\n",
    "#### Training error\n",
    "- Compute training error\n",
    "  1. Define a loss function $L(y, f_{\\hat{w}}(X))$\n",
    "    - E.g., squared error, absolute error, RMSE(root mean squared error)..\n",
    "  2. Training error\n",
    "    - avg.loss on houses in training set\n",
    "    - $\\frac{1}{N} \\sum L(y, f_{\\hat{w}}(X))$\n",
    "    \n",
    "- Training error vs. model complexity\n",
    " - training error decreases significantly with model complexity\n",
    "- Is training error a good measure of predictive performance?\n",
    " - issue: Training error is overly optimisic because $\\hat{w}$ was fit to training data\n",
    " - *having small training error does not imply having good predictive performance*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalization (true) error\n",
    "- Formally: generalization error = $E_{X, y}[L(y, f_{\\hat{w}}(X))]$\n",
    " - average over all possible (X, y) paires weighted by how likely each is\n",
    "- Generalization error vs. model complexity\n",
    " - generalization error is going down, and the we get to a point where the error starts increasing\n",
    " - Can't compute! (It's ideal)\n",
    " \n",
    "#### Test error\n",
    "- Approximating generalization error\n",
    "- Hold out some $(X, y)$ that are *not* used for fitting the model\n",
    "- test error\n",
    " - avg.loss on houses in test set\n",
    " - $\\frac{1}{N_{test}} \\sum L(y, f_{\\hat{w}}(X))$\n",
    " - $\\hat{w}$ minimizes RSS of training data!\n",
    "- Training, true, & test error vs. model complexity\n",
    " - test error is a noisy approximation of generalization error\n",
    "\n",
    "#### Overfitting\n",
    "- If there exists a model iwht estimated params $w'$ such that\n",
    " - training error($\\hat{w}$) < training error($w'$)\n",
    " - test error($\\hat{w}$) > test error($w'$)\n",
    " \n",
    "<img src=\"./figures/w3-f1.png\" width=400>\n",
    "\n",
    "#### Training/test split\n",
    "- Too few training set\n",
    " - $\\hat{w}$ poorly estimated\n",
    "- Too few test set\n",
    " - test error bad approximation of generalization error\n",
    "- Rule of thumb\n",
    " - Typically, just enough test points to form a reasonable estimate of generalization error\n",
    " - If this leaves too few for training, other methods like *cross validation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3 sources of error and the bias-variance tradeoff\n",
    "\n",
    "1. Noise\n",
    "2. Bias\n",
    "3. Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noise\n",
    "- Data inherently noisy\n",
    "- $y_i = f_{w(true)}(X_i) + \\epsilon_i$\n",
    "- **Irreducible error**: $\\epsilon_i$\n",
    "<img src=\"./figures/w3-f2.png\" width=400>\n",
    "\n",
    "#### Bias\n",
    "- Assume we fit a constant function(=simple model, low complexity model)\n",
    " - Over all possible size $N$ training sets, what do I expect my fit to be?\n",
    " - Bias is the difference between this average fit and the true function\n",
    " - $Bias(X) = f_{w(true)}(X) - f_{\\bar{w}}(X)$\n",
    "- *Is our approach flexible enough to caputre $f_{w(true)}(X)$? If not, error in predictions*\n",
    " - **low complexity -> high bias**\n",
    "\n",
    "<img src=\"./figures/w3-f3.png\" width=350 align=left>\n",
    "<img src=\"./figures/w3-f4.png\" width=400 align=right>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance\n",
    "- How much do specific fits vary from the expected fit?\n",
    " - $Var(X) = f_{\\hat{w}}(X) - f_{\\bar{w}}(X)$\n",
    "- *Can specific fits vary widely? If so, erratic predictions*\n",
    " - **low complexity -> low variance**\n",
    "\n",
    "<img src=\"./figures/w3-f5.png\" width=350 align=left>\n",
    "<img src=\"./figures/w3-f6.png\" width=380 align=right>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assume we fit a high-order polynomial(=high complexity model)\n",
    " - The variation between these fits, really large\n",
    "\n",
    "- **high complexity -> high variance**\n",
    "\n",
    "<img src=\"./figures/w3-f7.png\" width=350 align=left>\n",
    "<img src=\"./figures/w3-f8.png\" width=350 align=right>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **high complexity -> low bias**\n",
    "\n",
    "<img src=\"./figures/w3-f9.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias-variance tradeoff\n",
    "- Bias decrease, Variace increase at high complexity\n",
    "- $MSE = bias^2 + variance$\n",
    "- the goal is finding a sweet spot\n",
    "- Just like with generalization error, we **cannot** compute bias and variance\n",
    " - because we cannot compute true function and cannot get all data in the world\n",
    "<img src=\"./figures/w3-f10.png\" width=400>\n",
    "\n",
    "#### Error vs. amount of data (for a fixed model complexity)\n",
    "- true error\n",
    " - $\\hat{w}$ is not approximated well from few points\n",
    " - decrease error until meet the limit(bias + noise)\n",
    "- training error\n",
    " - with few data points, a fixed complexity model can fit these points reasonably well\n",
    "- In the limit, the curve will flatten out to how well model can fit true relationship $f_{true}$\n",
    "- In the limit, true error = training error\n",
    "<img src=\"./figures/w3-f11.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### OPTIONAL ADVANCED MATERIAL: Formally defining and deriving the 3 sources of error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accounting for training set randomness\n",
    "- Training set was just a random sample of N houses sold\n",
    "- What if *N other houses* had been sold and recorded?\n",
    "- Ideally, want performance `averaged over all possible training sets` of size N\n",
    "<img src=\"./figures/w3-f12.png\" width=400>\n",
    "\n",
    "#### Expected prediction error\n",
    "- $E_{\\text{training set}}[\\text{generalization error of }\\hat{w}(\\text{training set})]$\n",
    " - $E_{\\text{training set}}$: averaging over all training sets (weighted by how likely each is)\n",
    " - $\\hat{w}$: parameters fit on a specific training set\n",
    "\n",
    "- Prediction error at target input:\n",
    "  1. Loss at target $X_{t}$ (e.g. 2640 sq.ft.)\n",
    "  2. Squared error loss $L(y, f_{\\hat{w}}(X)) = (y - f_{\\hat{w}}(X))^2$\n",
    "  \n",
    "#### Sum of 3 sources of error\n",
    "- **Average prediction error at $X_t$**\n",
    " - $\\sigma^2 + [bias(f_{\\hat{w}}(X_t))]^2 + var(f_{\\hat{w}}(X_t))$\n",
    "\n",
    "<img src=\"./figures/w3-f13.png\" width=400>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error variance of the model ($\\sigma^2$)\n",
    "- $y = f_{w(true)}(X) + \\epsilon$\n",
    " - $\\epsilon$: all other factors out there in the world are captured by noise term\n",
    " - $E[\\epsilon] = 0$\n",
    " \n",
    "- Error variance $\\sigma^2$\n",
    " - the spread of noise you're likely to see at any point in the input space.\n",
    "\n",
    "- `Irreducible error`\n",
    " - no control over it no matter how complicated and interesting of a model, we specify our algorithm for fitting that model\n",
    " - we can't do anything about the fact that we're using $X$ for our prediction. But there's just inherently some noise in how our observations are generated in the world.\n",
    "\n",
    "\n",
    "<img src=\"./figures/w3-f14.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias of function estimator\n",
    "- Average estimated function $f_{\\bar{w}}(X)$\n",
    " - averaged over all possible training data sets of size $N$ that I might get.\n",
    "- True function $f_{w}(X)$\n",
    "\n",
    "- Bias\n",
    " - $bias(f_{w(true)}(X_t)) = f_{w}(X_t) - f_{\\bar{w}}(X_t)$\n",
    " - when it comes in as error term $\\sigma^2 + [bias(f_{\\hat{w}}(X_t))]^2 + var(f_{\\hat{w}}(X_t))$,\n",
    " - `bias squared` because of scaling with the other terms($\\sigma^2, var(f_{\\hat{w}}(X_t))$)\n",
    "\n",
    "<img src=\"./figures/w3-f15.png\" width=370 align=left>\n",
    "<img src=\"./figures/w3-f16.png\" width=350 align=right>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance of function estimator\n",
    "\n",
    "- Variance\n",
    " - Over all possible fits, How much do they deviate from expected fit\n",
    " - how much variation is there in the training dataset specific fits across all training datasets we might see?\n",
    " - $var(f_{\\hat{w}}(X_t)) = E_{train}[(f_{\\hat{w}(train)}(X_t) - f_{\\bar{w}}(X_t))^2]$\n",
    " \n",
    "<img src=\"./figures/w3-f17.png\" width=370 align=left>\n",
    "<img src=\"./figures/w3-f18.png\" width=350 align=right>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deriving expected prediction error\n",
    "- Expected prediction error\n",
    " - $E_{\\text{train}}[\\text{generalization error of }\\hat{w}(\\text{train})]$\n",
    " - $E_{\\text{train}}[E_{X, y}[L(y, f_{\\hat{w}(train)}(X))]]$\n",
    "\n",
    "\n",
    "1. Look at target $X_{t}$\n",
    "2. Consider $L(y, f_{\\hat{w}}(X)) = (y - f_{\\hat{w}}(X))^2$\n",
    "\n",
    "\n",
    "- **Expected prediction error at $X_t$**\n",
    " - = $E_{\\text{train}, y_t}[(y_t - f_{\\hat{w}(train)}(X_t))^2]$\n",
    "\n",
    "<img src=\"./figures/w3-f18.png\" width=400>\n",
    "<img src=\"./figures/w3-f19.png\" width=400>\n",
    "<img src=\"./figures/w3-f20.png\" width=400>\n",
    "<img src=\"./figures/w3-f21.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Data split for model selection\n",
    "\n",
    "#### The regression/ML workflow\n",
    "1. Model selection\n",
    " - Often, need to **choose tuning parameters** $\\lambda$ controlling model complexity (e.g., degree of polynomial)\n",
    "2. Model assessment\n",
    " - Having selected a model, **assess the generalization error**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothetical implementation (`Overly optimistic!`)\n",
    "\n",
    "<img src=\"./figures/w3-f23.png\" width=300>\n",
    "\n",
    "1. Model selection\n",
    " - For each considered model complexity $\\lambda$:\n",
    "   - Estimate parameters $\\hat{w}_{\\lambda}$ on **training data**\n",
    "   - Assess performance of $\\hat{w}_{\\lambda}$ on **test data**\n",
    "   - Choose $\\lambda^{*}$ to be $\\lambda$ with **lowest test error**\n",
    "\n",
    "\n",
    "2. Model assessment\n",
    " - Compute test error of $\\hat{w}_{\\lambda^*}$ (fitted model for selected complexity $\\lambda^*$ to approx. generalization error\n",
    " \n",
    "\n",
    "- **Issue:** Just like fitting $\\hat{w}$ and assessing its performance both on training data\n",
    " - $\\lambda^*$ was selected to minimize **test error** (i.e., $\\lambda^*$ was fit on test data)\n",
    " - If test data is not representative of the whole world, then $\\hat{w}_{\\lambda^*}$ will typically perform worse than **test error** indicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical implementation(`Solution`)\n",
    "\n",
    "<img src=\"./figures/w3-f24.png\" width=300>\n",
    "\n",
    "- **Solution :** Create two \"test\" sets!\n",
    " - Selecte $\\lambda^*$ such that $\\hat{w}_{\\lambda^*}$ minimizes error on **validation set**\n",
    " - Approximate generalization error of $\\hat{w}_{\\lambda^*}$ using **test set**\n",
    "- Doing the split between training set, validation set, and test set?\n",
    " - No hard and fast rule, no one answer\n",
    " - Typical splits are 80:10:10, 50:25:25 etc.\n",
    " - It's assuming that you have enough data to do this type of split and still get reasonable estimates of your model parameters reasonable notions of how different model complexities compare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### What you can do now...\n",
    "- Describe what a loss function is and give examples\n",
    "- Contrast training, generalization, and test error\n",
    "- Compute training and test error given a loss function\n",
    "- Discuss issue of assessing performance on trainng set\n",
    "- Describe tradeoffs in forming training/test splits\n",
    "- List and interpret the 3 sources of avg. prediction error\n",
    " - Irreducible error, bias, and variance\n",
    "- Discuss issue of selecting model complexity on test data and then using test error to assess generalization error\n",
    "- Motivate use of a validation set for selecting tuning parameters (e.g., model complexity)\n",
    "- Describe overall regression workflow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
