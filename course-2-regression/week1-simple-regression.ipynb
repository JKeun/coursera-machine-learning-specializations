{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression fundamentals: data, model, task\n",
    "- **Data**\n",
    " - Input vs. Output:\n",
    "   - $y$ is the quantity of interest\n",
    "   - assume $y$ can be predicted from $x$\n",
    "- **Model**\n",
    " - $f(x)$ : expected relationship between $x$ and $y$\n",
    " - **Regressions model:**\n",
    "   - $y_i = f(x_i) + \\epsilon_i$\n",
    "   - $E[\\epsilon_i] = 0$\n",
    "     - equally likely that error is $+$ or $-$\n",
    "     - $y_i$ is equally likely to be above or below $f(x_i)$\n",
    "\n",
    "<img src=\"./figures/w1-f1.png\" width=600>\n",
    "\n",
    "- **Task 1 - Which model $f(x)$?**\n",
    " - average model\n",
    " - linear relationship model\n",
    " - quadratic fit\n",
    " - polynomial fit\n",
    " - ...\n",
    "\n",
    "- **Task 2 - For a given model $f(x)$, estimate function $\\hat{f}(x)$ from data**\n",
    " - Assume model $f(x)$ is a quadratic function\n",
    " - estimated quadratic fit $\\hat{f}(x)$ from data, different fit, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression ML block diagram\n",
    "- `Training data` -> `Feature extraction` -> $x$\n",
    "- $x$ -> `ML model` ( regression ) -> $\\hat{y}$ : predicted values\n",
    "- `Quality metric` ( $y$: actual value s & $\\hat{y}$ ) : error in our predicted values -> `ML algorithm` -> $\\hat{f}$ : estimated function fit from data -> `ML model`\n",
    " - loop, updating the weights or model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple linear regression\n",
    "- $y_i = w_0 + w_1x_i + \\epsilon_i$\n",
    " - $f(x) = w_0 + w_1x$\n",
    " - parameters: regression coefficients ($w_0, w_1$)\n",
    " \n",
    " \n",
    "- **Fitting a line to data**\n",
    " - RSS: Residual sum of squares\n",
    "   - `error` is a part of model, `residual` is the difference between a predictino and an actual value\n",
    " - $RSS(w_0, w_1) = \\sum_{i=1}^{N}{(y_i - [w_0 + w_1x_i])^2}$\n",
    " - minimize cost over all possible $w_0, w_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization 1-dim\n",
    "- Concave vs convex\n",
    " - concave : line lies below $g(w)$ everywhere\n",
    " - convex : line lies above $g(w)$ everywhere\n",
    "<img src=\"./figures/w1-f2.png\" width=500>\n",
    "- weight update\n",
    " - minimize: $w^{t+1} = w^{t} - \\eta\\frac{dg(w)}{dw}$\n",
    " - hill climbing\n",
    "   - if $\\frac{dg(w)}{dw} > 0$ -> $w$ is decreased\n",
    "   - if $\\frac{dg(w)}{dw} < 0$ -> $w$ is increased\n",
    "<img src=\"./figures/w1-f3.png\" width=500>\n",
    "- stepsize\n",
    " - fixed stepsize\n",
    " - decreasing stepsize(=stepsize schedule) : common choice\n",
    "   - $\\eta_t = \\frac{\\alpha}{t}$\n",
    "   - $\\eta_t = \\frac{\\alpha}{\\sqrt{t}}$\n",
    "- convergence criteria\n",
    " - $|\\frac{dg(w)}{dw}| < \\epsilon$, $\\epsilon$ is threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization multi-dims\n",
    "- derivatives in multi-dim\n",
    " - $\\nabla{g(w)} = \\begin{bmatrix} \\frac{\\partial{g}}{w_0} \\\\ \\frac{\\partial{g}}{w_1} \\\\ \\vdots \\\\ \\frac{\\partial{g}}{w_p} \\end{bmatrix}$\n",
    "<img src=\"./figures/w1-f4.png\" width=500>\n",
    "- gradient descent\n",
    " - minimize: $w^{t+1} = w^{t} - \\eta\\nabla{g(w^{t})}$\n",
    " - $\\nabla{g(w)} < \\epsilon$, $\\epsilon$ is threshold\n",
    "<img src=\"./figures/w1-f5.png\" width=500> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the least squared line\n",
    "- gradient of RSS\n",
    " - $RSS(w_0, w_1) = \\sum_{i=1}^{N}{[y_i - (w_0 + w_1x_i)]^2}$\n",
    " - $\\nabla{RSS(w_0, w_1)} = \\begin{bmatrix} -2\\sum_{i=1}^{N}{[y_i - (w_0 + w_1x_i)]} \\\\ -2\\sum_{i=1}^{N}{[y_i - (w_0 + w_1x_i)]x_i} \\end{bmatrix}$\n",
    "- Approach 1 - closed form solution\n",
    "<img src=\"./figures/w1-f7.png\" width=500>\n",
    "- Approach 2 - gradient descent\n",
    "<img src=\"./figures/w1-f8.png\" width=500>\n",
    "<img src=\"./figures/w1-f9.png\" width=500>\n",
    "- Comparing the approaches\n",
    " - for most ML problems, cannot solve `gradient=0`\n",
    " - even if solving `gradient=0` is feasible, `gradient descent` can be more efficient\n",
    " - `gradient descent` relies on choosing `stepsize`and `convergence`criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([0, 1, 2, 3, 4])\n",
    "y = np.array([1, 3, 7, 13, 21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.0, -1.0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# approach 1 - using sums\n",
    "w1 = (sum(X*y) - (sum(X)*sum(y))/len(X)) / (sum(X**2) - (sum(X)*sum(X))/len(X))\n",
    "w0 = (sum(y) - w1*sum(X)) / len(X)\n",
    "\n",
    "w1, w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.0, -1.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# approach 1 - using means\n",
    "w1 = (np.mean(X*y) - np.mean(X)*np.mean(y)) / (np.mean(X**2) - np.mean(X)*np.mean(X))\n",
    "w0 = np.mean(y) - w1*np.mean(X)\n",
    "\n",
    "w1, w0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall that:**\n",
    "\n",
    "- The derivative of the cost for the intercept is the sum of the errors\n",
    "- The derivative of the cost for the slope is the sum of the product of the errors and the input\n",
    "\n",
    "\n",
    "**The algorithm**\n",
    "\n",
    "In each step of the gradient descent we will do the following:\n",
    "1. Compute the predicted values given the current slope and intercept\n",
    "2. Compute the prediction errors (prediction - Y)\n",
    "3. Update the intercept:\n",
    " - compute the derivative: sum(errors)\n",
    " - compute the adjustment as step_size times the derivative\n",
    " - decrease the intercept by the adjustment\n",
    "4. Update the slope:\n",
    " - compute the derivative: sum(errors*input)\n",
    " - compute the adjustment as step_size times the derivative\n",
    " - decrease the slope by the adjustment\n",
    "5. Compute the magnitude of the gradient\n",
    "6. Check for convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.99373992,  4.00406416,  9.00186824, 13.99967233, 18.99747641]),\n",
       " -0.9942069818917416,\n",
       " 4.997967918970868,\n",
       " 78)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# approach 2\n",
    "import numpy as np\n",
    "\n",
    "def gradient_descent(X, y, initial_intercept=0, inital_slope=0, step_size=0.05, tolerance=0.01):\n",
    "    w0 = initial_intercept\n",
    "    w1 = inital_slope\n",
    "\n",
    "    iterations = 0\n",
    "    while True:\n",
    "        preds = w0 + w1*X\n",
    "        errors = y - preds\n",
    "        sum_errors = np.sum(errors)\n",
    "        sum_errors_X = np.sum(errors*X)\n",
    "        magnitude = np.sqrt(sum_errors**2 + sum_errors_X**2)\n",
    "        \n",
    "        w0 += step_size * sum_errors\n",
    "        w1 += step_size * sum_errors_X\n",
    "        iterations += 1\n",
    "        \n",
    "        if magnitude < tolerance:\n",
    "            break\n",
    "    \n",
    "#         print(preds, w0, w1, magnitude)\n",
    "    return preds, w0, w1, iterations\n",
    "\n",
    "gradient_descent(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What you can do now..\n",
    "- Describe the input (features) and ouput (real-valued predictions) of a regression model\n",
    "- Calculate a goodness-of-fit metric (e.g., RSS)\n",
    "- Estimate model parameters to minimize RSS using gradient descent\n",
    "- Interpret estimated model parameters\n",
    "- Exploit the estimated model to form predictions\n",
    "- Discuss the possible influence of high leverage points\n",
    "- Describe intuitively how fitted line might change when assuming different goodness-of-fit metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
