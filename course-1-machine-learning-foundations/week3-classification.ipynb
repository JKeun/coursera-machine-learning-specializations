{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting sentiment by topic: An intelligent restaurant review system\n",
    "- All reviews for restaurant\n",
    "- Break all reviews into sentences\n",
    "- Select sentences about \"sushi\"\n",
    "- Sentence Sentiment Classifier\n",
    "- Average predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier applications\n",
    "- Sentence from review\n",
    " - Input: $x$\n",
    "- Classifier MODEL\n",
    " - Output: $y$ ( Predicted class: + or - )\n",
    "- Output $y$ has more than 2 categories\n",
    "- Spam filtering\n",
    " - e mail -> Spam or No Spam\n",
    "- Image classification\n",
    " - Image pixels -> Predicted object\n",
    "- Personalized medical diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear classifiers\n",
    "- Simple threshold classifier\n",
    " - List of positive words: great, awesome, good, amazing..\n",
    " - List of negative words: bad, terrible, disgusting, sucks..\n",
    " - Count positive & negative words in sentence\n",
    "   - if # of positive words > # of negative words:\n",
    "   - $\\hat{y} = +$\n",
    "- Problems with threshold classifier\n",
    " - How do we get list of positive/negative words?\n",
    " - Words have different degrees of sentiment:\n",
    "   - Great > good\n",
    "   - How do we weigh differenct words?\n",
    " - Single words are not enough:\n",
    "   - Good -> Positive\n",
    "   - Not good -> Negative\n",
    "- Will use training data to learn a weight for each word\n",
    "\n",
    "\n",
    "$\\text{Score}(x) = \\text{weighted count of words in sentence}$\n",
    "- If $\\text{Score}(x) > 0:$\n",
    "  - $\\hat{y} = +$\n",
    "- Else:\n",
    "  - $\\hat{y} = -$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision boundaries\n",
    "- $\\text{Score}(x) = 1.0 \\cdot \\text{#awesome} - 1.5 \\cdot \\text{#awful}$\n",
    " - $\\text{Score}(x) < 0$ : Negative area\n",
    " - **$\\text{Score}(x) = 1.0 \\cdot \\text{#awesome} - 1.5 \\cdot \\text{#awful} = 0$ : Decision boundary**\n",
    " - $\\text{Score}(x) > 0$ : Positive area\n",
    "- Decision boundary separates positive & negative predictions\n",
    " - For linear classifiers:\n",
    "   - When 2 wieghts are non-zero -> **line**\n",
    "   - When 3 wieghts are non-zero -> **plane**\n",
    "   - When many wieghts are non-zero -> **hyperplane**\n",
    " - For more general classifiers\n",
    "   - **more complicated shapes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a classifier = Learning the weights\n",
    "- Test example\n",
    " - -> `Learned classifier`\n",
    " - -> `Hide label`\n",
    " - Check Correct & Mistakes\n",
    "- Classification error & accuracy\n",
    " - Error measures fraction of mistakes\n",
    "$$\\text{error} = \\frac{\\text{# of mistakes}}{\\text{Total # of sentences}}$$\n",
    "   - Best possible value is 0.0\n",
    " - Often, measure **accuracy**\n",
    "   - Fraction of correct predictions:\n",
    "$$\\text{accuracy} = \\frac{\\text{# of corrects}}{\\text{Total # of sentences}}$$\n",
    "   - Best possible value is 1.0\n",
    "   - error = 1 - accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random guess\n",
    "- For binary classification;\n",
    " - Half the time, you'll get it right! (on average)\n",
    " - accuracy = 0.5\n",
    "- For $k$ classes, accuracy = $1/k$\n",
    "- Is a classifier with 90% accuracy good? Depends...\n",
    " - \"90% emails sent are spam!\"\n",
    " - Predicting every email is spam gets you 90% accuracy!!\n",
    " - `Mafority class prediction`\n",
    " - Amazing performance when there is class imbalance\n",
    "- So, always be digging in and asking the hard questions about reported accuracies\n",
    " - Is there class imbalance?\n",
    " - How does it compare to a simple, baseline approach?\n",
    "   - Random guessing\n",
    "   - Mafority class\n",
    "   - ...\n",
    " - Most importantly: What accuracy does my application need?\n",
    "   - What is good enough for my user's experience?\n",
    "   - What is the impoact of the mistakes we make?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "\n",
    "- binary classification\n",
    "\n",
    "<img src=\"https://docs.wso2.com/download/attachments/48292444/Binary_Classification_Matrix_Definition.png?version=1&modificationDate=1447821750000&api=v2\">\n",
    "\n",
    "- False Negative(FN) & False Positive(FP) have different impact!\n",
    " - Spam filtering : FN -> Annoying // **FP -> Email lost (high risk)**\n",
    " - Medical diagnosis : **FN -> Disease not treated // FP -> Wasteful treatment**\n",
    " \n",
    " \n",
    "- multiclass classification\n",
    "\n",
    "<img src=\"https://docs.wso2.com/download/attachments/48292444/Multiclass_Classification_Matrix_Definition.png?version=1&modificationDate=1447821750000&api=v2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curves: How much data do I need?\n",
    "- The more the merrier\n",
    " - But data quality is most important factor\n",
    "- Theoretical techniques sometimes can bound how much data is needed\n",
    " - Typically too loose for practical application\n",
    " - But provide guidance\n",
    "- In practice:\n",
    " - More complex models require more data\n",
    " - Empirical anaysis can provide guidance\n",
    " \n",
    " \n",
    "- Learning curves with amount training data\n",
    "\n",
    "<img src=\"https://kwtrnka.files.wordpress.com/2015/05/learning_curve_basic_3.png\">\n",
    "\n",
    "\n",
    "- **Bias of model**\n",
    " - Even with infinite data, test error will not go to zero\n",
    "- More complex models tend to have less bias\n",
    " - Sentiment classifier using single words can do OK but..\n",
    " - Never classify correctly: \"The sushi was *not good*.\"\n",
    " - More complex model: consider pairs of words (bigrams)\n",
    " - Less bias -> potentially more accurate, needs more data to learn\n",
    "- More complex models with less data is not going to do well, because it has more parameters to fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class probabilities\n",
    "-  Many classifiers provide a confidence level:\n",
    " - $P(y|x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression ML block diagram\n",
    "- `Training Data` -> `Featrue extraction` -> $x$\n",
    "- $x$ -> `ML model` ( $\\hat{w}$ ) -> $\\hat{y}$\n",
    "- `Quality metric` -> `ML algorithm` -> RSS ( $y-\\hat{y}$ ) -> $\\hat{w}$\n",
    " - loop, updating the weights or model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification ML block diagram\n",
    "- `Training Data` -> `Featrue extraction` -> $x$ (word counts)\n",
    "- $x$ -> `ML model` ( $\\hat{w}$ ) -> $\\hat{y}$ (predicted sentiment)\n",
    "- `Quality metric` -> `ML algorithm` -> accuracy ( $y, \\hat{y}$ ) -> $\\hat{w}$ (weights for each word)\n",
    " - loop, updating the weights or model parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
